from langchain_community.document_loaders import TextLoader         #Carrega fitxers de text
from langchain.text_splitter import RecursiveCharacterTextSplitter  #Divideix el text en fragments m√©s petits "Chunks"

from langchain_huggingface import HuggingFaceEmbeddings             #Utilitza models "SBERT" per convertir cada chunk en vector
from langchain_community.vectorstores import Chroma                 #S'encarrega de guardar els vectors

from langchain_ollama import OllamaLLM                             #Envia prompts al model local

from langchain.chains import RetrievalQA                            #Uneix el sistema de recuperaci√≥ "Chroma" amb "Ollama" per poder fer preguntes

# Carregar l'arxiu de text:
def carregar_document(ruta_txt: str):                           #La part de str indica a VisualStudio que sera un "string" √©s a dir, text, per a que aix√≠ pugui detectar els errors m√©s facilment
    loader = TextLoader(ruta_txt, encoding='utf-8')             #Crea una variable que sap llegir l'arxiu, la part d'encodig fa que entengui m√©s tipus de car√†cters
    documents = loader.load()                                   #Crea una llista per cada document que detecta
    return documents

# Dividir el text en fragments m√©s petits "chunks":
def dividir_chunks(documents, chunk_size: int = 1500, chunk_overlap: int = 500):         # La superposicio indica que el seguent fragment agafi els √∫ltims 200 car√†cters per tal de no tallar frases per la meitat
    splitter = RecursiveCharacterTextSplitter(                                           # Separa el text segons el que haguem definit en els par√†metres
        chunk_size = chunk_size,                                                         # Defineix el par√†metre de la funci√≥ com un objecte que t√© la llibreria
        chunk_overlap = chunk_overlap
    )
    chunks = splitter.split_documents(documents)                                         # Indiquem quin text ha de separar i utilitza una funci√≥ per mantenir millor el context
    return chunks


# Convertir els chunks en vectors 
def crear_vectors(chunks, persist_directory: str = "vectordb"):                                     # Persist_directory √©s la carpeta on Chroma guarda els fitxers en el disc
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")   # Crea un model que transforma el text en vectors num√®rics 
    vectordb = Chroma.from_documents(                                                               # Crea una base de dades de vectors a partir dels chunks utilitzant el model especificat abans, i especifica la carpeta on guarda els fitxers
        documents=chunks,
        embedding=embedding_model,
        persist_directory=persist_directory
    )
    return vectordb                                                                                 # Aix√≤ el que retorna √©s la ubicaci√≥ on Chroma ha guardat aquests vectors


# Connectar Ollama amb el model que hem triat
def connectar_ollama(model: str = "llama3:8b"):
    llm = OllamaLLM(model=model)                       # Indica a Ollama quin model ha d'utlitzar, aquest ja hauria d'estar instal¬∑lat localment a l'ordinador
    return llm

# Crear la cadena RAG, unint la base vectorial de Chroma amb el model de Llama
def crear_RAG(llm, vectordb):
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})             # Busca els tro√ßos de text m√©s semblants a la pregunta
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    return qa_chain

# Funci√≥ que fa la pregunta
def fer_pregunta(qa_chain, pregunta: str):
    resultat = qa_chain.invoke({"query": pregunta})
    return resultat

document = carregar_document("Documents/TDR Informaci√≥ (1).txt")

chunks = dividir_chunks(document)

vectordb = crear_vectors(chunks)

llm = connectar_ollama()

qa_chain = crear_RAG(llm, vectordb)

pregunta = "Quin √©s el teu prop√≤sit com a intel¬∑lig√®ncia artificial? Sobre quines drogues tens informaci√≥ i sobre quines no?"

print("üü° Pregunta enviada al model...")
resultat = fer_pregunta(qa_chain, pregunta)
print("‚úÖ Resposta rebuda!")
print(resultat["result"])

# Mostrem els fragments de text utilitzats pel model per respondre
print("\n Fragments utilitzats pel model:")
for i, doc in enumerate(resultat["source_documents"]):
    print(f"\n--- Fragment {i+1} ---")
    print(doc.page_content[:1000])  # Mostra els primers 1000 car√†cters del chunk
